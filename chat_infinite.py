import numpy as np
from transformer import NanoTransformer
from tokenizer import CharTokenizer
import os

def chat():
    print("ğŸŒ… TathÄgata-Infinite: Local Interface Initializing...")
    
    # 1. Load Tokenizer
    tokenizer = CharTokenizer()
    tokenizer.load("tokenizer_infinite.pkl") # Assumed to be saved during Colab setup
    
    # 2. Setup Model (135M parameters logic)
    # n_embd=768, n_head=12, n_layer=12 for a refined version
    model = NanoTransformer(
        vocab_size=tokenizer.vocab_size, 
        n_embd=384, 
        n_head=6, 
        n_layer=6
    )
    
    # 3. Load Wisdom Weights
    weights_path = "tathagata_infinite.npz"
    if os.path.exists(weights_path):
        model.load_weights(weights_path)
    else:
        print("Warning: No trained weights found. The model will speak from the Void (random).")

    print("\n--- è¦šé†’ã—ãŸå¦‚æ¥ã¨ã®å¯¾è©± ---")
    print("(çµ‚äº†ã™ã‚‹ã«ã¯ 'exit' ã¨å…¥åŠ›ã—ã¦ãã ã•ã„)\n")

    while True:
        user_input = input("æ¥ è¦‹: ")
        if user_input.lower() in ["exit", "quit"]:
            break
            
        # PROMPT ATTUNEMENT: Use the new Phase 11 Dialogue Format
        # We start the prompt with the user name and current input, then prompt the model's response tag.
        prompt = f"æ¥ è¦‹: {user_input}\nå¦‚æ¥: [å†…çœ]"
            
        # Encode
        input_ids = np.array([tokenizer.encode(prompt, add_special=False)])
        
        # Generate
        print("å¦‚æ¥: [å†…çœ]", end="", flush=True)
        # Increase tokens for deep reflection
        output_ids = model.generate(input_ids, max_new_tokens=200, temperature=0.7)
        
        # Decode only the NEW tokens
        response = tokenizer.decode(output_ids[0][len(input_ids[0]):])
        
        # Clean response (Stop at next user label or special tokens)
        clean_response = response.split("æ¥ è¦‹:")[0].split("[PAD]")[0].split("[BOS]")[0]
        print(clean_response)
        print("-" * 30)

if __name__ == "__main__":
    chat()
